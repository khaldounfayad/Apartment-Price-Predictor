{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFz-nSR42xhS",
        "outputId": "cbbb4eaa-7257-4195-d09f-267d9255b52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.14.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
      ],
      "metadata": {
        "id": "VHj3p-Fy3U5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataPipeline(df):\n",
        "\n",
        "\n",
        "\n",
        "    df_live, df_backup = train_test_split(df,test_size=0.3, random_state=42)\n",
        "\n",
        "    num_cols = df_live.select_dtypes([np.number]).columns\n",
        "    df_nums = df_live[num_cols].reset_index(drop=True)\n",
        "\n",
        "    X = df_nums.loc[:,df_nums.columns != 'price']\n",
        "    y = df_nums['price'].values\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    X_normalized = scaler.fit_transform(X)\n",
        "    X_normalized_df = pd.DataFrame(X_normalized, columns=X.columns)\n",
        "\n",
        "    selected_columns_2 = ['squareMeters', 'longitude']\n",
        "    selected_columns_3 = ['squareMeters', 'longitude', 'poiCount']\n",
        "    selected_columns_5 = ['squareMeters', 'longitude', 'poiCount', 'rooms', 'centreDistance']\n",
        "    selected_columns_7 = ['squareMeters', 'longitude', 'poiCount', 'rooms', 'centreDistance', 'clinicDistance', 'kindergartenDistance']\n",
        "\n",
        "    X2=X_normalized_df[selected_columns_2]\n",
        "    X3=X_normalized_df[selected_columns_3]\n",
        "    X5=X_normalized_df[selected_columns_5]\n",
        "    X7=X_normalized_df[selected_columns_7]\n",
        "\n",
        "    FeatureDividedData = {'X2':X2,'X3':X3,'X5':X5,'X7':X7}\n",
        "\n",
        "    workableData = {'X':X,'y':y,'X_normalized':X_normalized,'X_normalized_df':X_normalized_df,'FeatureDividedData':FeatureDividedData}\n",
        "\n",
        "    dataJourney={'df_live':df_live,'df_backup':df_backup,'df_nums':df_nums,'scaler':scaler,'workableData':workableData}\n",
        "\n",
        "    return dataJourney"
      ],
      "metadata": {
        "id": "YyI_zjC52-K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getData():\n",
        "    df_august = pd.read_csv('https://raw.githubusercontent.com/WitoldSurdej/PFML/master/apartments_pl_2023_08.csv')\n",
        "    df_september = pd.read_csv('https://raw.githubusercontent.com/WitoldSurdej/PFML/master/apartments_pl_2023_09.csv')\n",
        "    df_october = pd.read_csv('https://raw.githubusercontent.com/WitoldSurdej/PFML/master/apartments_pl_2023_10.csv')\n",
        "\n",
        "    df_august['Month'] = 0\n",
        "    df_september['Month'] = 1\n",
        "    df_october['Month'] = 2\n",
        "\n",
        "    frames = [df_august, df_september, df_october]\n",
        "    df = pd.concat(frames)\n",
        "\n",
        "    dataJourney = dataPipeline(df)\n",
        "\n",
        "    return dataJourney"
      ],
      "metadata": {
        "id": "KBMMWJRN3J3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model building function\n",
        "def build_model(hp):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Dense(units=hp.Int('units', min_value=2, max_value=22, step=10), activation='sigmoid'))\n",
        "    model.add(layers.Dense(units=hp.Int('units', min_value=2, max_value=22, step=10), activation='sigmoid'))\n",
        "\n",
        "\n",
        "    model.add(layers.Dense(1))  # Output layer\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3])),\n",
        "                  loss='mse',\n",
        "                  metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def build_model2(hp):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Define the hyperparameter search space for the number of layers\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=6, step=2)\n",
        "\n",
        "    # Add a variable number of Dense layers based on the sampled value\n",
        "    for i in range(num_layers):\n",
        "        # For each layer, define the number of units (neurons) in the layer\n",
        "        units = hp.Int(f'units_{i}', min_value=2, max_value=22, step=10)\n",
        "        model.add(keras.layers.Dense(units=units, activation='sigmoid'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(keras.layers.Dense(units=1))  # Assuming a regression task with one output neuron\n",
        "\n",
        "    # Define the hyperparameter search space for the learning rate\n",
        "    learning_rate = hp.Choice('learning_rate', values=[1e-1, 1e-2, 1e-3])\n",
        "\n",
        "    # Compile the model with the sampled learning rate\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                  loss='mse',\n",
        "                  metrics=['mse'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "WSqkFFbv3jzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataFrame = getData()\n",
        "\n",
        "i=3\n",
        "\n",
        "X2=dataFrame['workableData']['FeatureDividedData'][f'X{i}']\n",
        "y=dataFrame['workableData']['y']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.2, random_state=42)\n",
        "X1_train1, X1_validation, y1_train1, y1_validation = train_test_split(X_train, y_train, test_size = 0.3, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "zXy0kapY4Gaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxAXZXrZ1tvx",
        "outputId": "17b21fce-5769-43eb-d650-27ccccff805f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reloading Tuner from my_dir/allIn/tuner0.json\n"
          ]
        }
      ],
      "source": [
        "# Instantiate a tuner (e.g., RandomSearch tuner)\n",
        "tuner = RandomSearch(\n",
        "    build_model2,\n",
        "    objective='val_loss',\n",
        "    max_trials=5,  # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir',\n",
        "    project_name='allIn')\n",
        "\n",
        "# Search for the best hyperparameter configuration\n",
        "tuner.search(x=X1_train1, y=y1_train1, epochs=5, validation_data=(X1_validation, y1_validation))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the best model using the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "\n",
        "# Explicitly call the build method of the model to finalize building\n",
        "best_model.build(input_shape=X_train.shape)\n",
        "\n",
        "# Now, let's summarize the best built model\n",
        "best_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RH33M-Pu7a_t",
        "outputId": "034fa1e0-6807-4b6c-8435-8924f5c5daea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_2 (Dense)             (29451, 2)                8         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (29451, 1)                3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11 (44.00 Byte)\n",
            "Trainable params: 11 (44.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8pKB-kF8dEs",
        "outputId": "c4b47c46-57db-4032-925e-74c880eb995b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in my_dir/allIn\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 2\n",
            "learning_rate: 0.1\n",
            "Score: 620113494016.0\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 22\n",
            "learning_rate: 0.01\n",
            "Score: 620400803840.0\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 2\n",
            "learning_rate: 0.01\n",
            "Score: 621341179904.0\n",
            "\n",
            "Trial 3 summary\n",
            "Hyperparameters:\n",
            "num_layers: 3\n",
            "units_0: 2\n",
            "learning_rate: 0.01\n",
            "units_1: 2\n",
            "units_2: 2\n",
            "Score: 621342359552.0\n",
            "\n",
            "Trial 4 summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 2\n",
            "learning_rate: 0.001\n",
            "units_1: 22\n",
            "units_2: 22\n",
            "Score: 621471531008.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def expDecay_of_learningRate(initial_learning_rate,final_learning_rate,total_epochs):\n",
        "\n",
        "  # Define the initial and final learning rates\n",
        "  initial_learning_rate = 1.0\n",
        "  final_learning_rate = 0.001\n",
        "  total_epochs = 100\n",
        "\n",
        "  # Calculate the decay factor\n",
        "  decay_factor = final_learning_rate / initial_learning_rate\n",
        "\n",
        "  # Calculate the decay rate per epoch\n",
        "  decay_rate = decay_factor ** (1 / total_epochs)\n",
        "\n",
        "\n",
        "\n",
        "  # Create a learning rate schedule with exponential decay\n",
        "  lr_schedule = ExponentialDecay(\n",
        "      initial_learning_rate,\n",
        "      decay_steps=decay_steps,\n",
        "      decay_rate=decay_rate,\n",
        "      staircase=True  # For discrete steps (staircase decay)\n",
        "  )\n",
        "\n",
        "  # Create an optimizer and use the learning rate schedule\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "  return optimizer\n"
      ],
      "metadata": {
        "id": "vFt-H-K382s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model2(hp):\n",
        "    model = keras.Sequential()\n",
        "\n",
        "    # Define the hyperparameter search space for the number of layers\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=12, step=2)\n",
        "\n",
        "    # Add a variable number of Dense layers based on the sampled value\n",
        "    for i in range(num_layers):\n",
        "        # For each layer, define the number of units (neurons) in the layer\n",
        "        units = hp.Int(f'units_{i}', min_value=2, max_value=52, step=10)\n",
        "        model.add(keras.layers.Dense(units=units, activation='sigmoid'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(keras.layers.Dense(units=1))  # Assuming a regression task with one output neuron\n",
        "\n",
        "    optimizer = expDecay_of_learningRate()\n",
        "    # Compile the model with the sampled learning rate\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='mse',\n",
        "                  metrics=['mse'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "05Lsl2haD578"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a tuner (e.g., RandomSearch tuner)\n",
        "tuner = RandomSearch(\n",
        "    build_model2,\n",
        "    objective='val_loss',\n",
        "    max_trials=5,  # Number of hyperparameter combinations to try\n",
        "    executions_per_trial=1,\n",
        "    directory='my_dir2',\n",
        "    project_name='init')\n",
        "\n",
        "# Search for the best hyperparameter configuration\n",
        "tuner.search(x=X1_train1, y=y1_train1, epochs=50, validation_data=(X1_validation, y1_validation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoIDh8gqErhz",
        "outputId": "edc3cb2c-8154-42a8-8557-0f31b67421fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 02m 05s]\n",
            "val_loss: 343182573568.0\n",
            "\n",
            "Best val_loss So Far: 145301176320.0\n",
            "Total elapsed time: 00h 09m 39s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best hyperparameters\n",
        "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the best model using the best hyperparameters\n",
        "best_model = tuner.hypermodel.build(best_hyperparameters)\n",
        "\n",
        "# Explicitly call the build method of the model to finalize building\n",
        "best_model.build(input_shape=X_train.shape)\n",
        "\n",
        "# Now, let's summarize the best built model\n",
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo68ljP6HNx9",
        "outputId": "e99f64f0-9bae-42b0-8050-0d8bbfc2947c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (29451, 42)               168       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (29451, 1)                43        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 211 (844.00 Byte)\n",
            "Trainable params: 211 (844.00 Byte)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.results_summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yknNOzoPHZKX",
        "outputId": "ca711a6d-4189-4649-a3d8-a9df49d3b2d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in my_dir2/init\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_loss\", direction=\"min\")\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 42\n",
            "units_1: 12\n",
            "units_2: 52\n",
            "units_3: 42\n",
            "units_4: 12\n",
            "Score: 145301176320.0\n",
            "\n",
            "Trial 01 summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "units_0: 32\n",
            "units_1: 52\n",
            "units_2: 32\n",
            "units_3: 12\n",
            "units_4: 42\n",
            "Score: 200791801856.0\n",
            "\n",
            "Trial 4 summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "units_0: 12\n",
            "units_1: 32\n",
            "units_2: 22\n",
            "units_3: 32\n",
            "units_4: 32\n",
            "units_5: 32\n",
            "units_6: 52\n",
            "Score: 343182573568.0\n",
            "\n",
            "Trial 3 summary\n",
            "Hyperparameters:\n",
            "num_layers: 7\n",
            "units_0: 42\n",
            "units_1: 12\n",
            "units_2: 2\n",
            "units_3: 42\n",
            "units_4: 32\n",
            "units_5: 2\n",
            "units_6: 2\n",
            "Score: 566641360896.0\n",
            "\n",
            "Trial 00 summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "units_0: 2\n",
            "units_1: 2\n",
            "units_2: 2\n",
            "units_3: 2\n",
            "units_4: 2\n",
            "Score: 608905003008.0\n"
          ]
        }
      ]
    }
  ]
}